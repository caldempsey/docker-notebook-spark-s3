{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49abe531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark==3.3.1\n",
      "  Using cached pyspark-3.3.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: py4j==0.10.9.5 in /opt/conda/lib/python3.11/site-packages (from pyspark==3.3.1) (0.10.9.5)\n",
      "Installing collected packages: pyspark\n",
      "  Attempting uninstall: pyspark\n",
      "    Found existing installation: pyspark 3.5.0\n",
      "    Can't uninstall 'pyspark'. No files were found to uninstall.\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "delta-spark 3.1.0 requires pyspark<3.6.0,>=3.5.0, but you have pyspark 3.3.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed pyspark-3.3.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: delta-spark==3.1.0 in /opt/conda/lib/python3.11/site-packages (3.1.0)\n",
      "Requirement already satisfied: pyspark<3.6.0,>=3.5.0 in /usr/local/spark/python (from delta-spark==3.1.0) (3.5.0)\n",
      "Requirement already satisfied: importlib-metadata>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from delta-spark==3.1.0) (6.8.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.11/site-packages (from importlib-metadata>=1.0.0->delta-spark==3.1.0) (3.17.0)\n",
      "Collecting py4j==0.10.9.7 (from pyspark<3.6.0,>=3.5.0->delta-spark==3.1.0)\n",
      "  Using cached py4j-0.10.9.7-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Using cached py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "Installing collected packages: py4j\n",
      "  Attempting uninstall: py4j\n",
      "    Found existing installation: py4j 0.10.9.5\n",
      "    Uninstalling py4j-0.10.9.5:\n",
      "      Successfully uninstalled py4j-0.10.9.5\n",
      "Successfully installed py4j-0.10.9.7\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Creating SparkSession...\n",
      "INFO:__main__:Setting up Spark configurations...\n"
     ]
    }
   ],
   "source": [
    "# ensure compatible versions of PySpark and Delta Lake are installed\n",
    "%pip install pyspark==3.3.1 \n",
    "%pip install delta-spark==3.1.0\n",
    "import logging\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import col\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logger.info(\"Creating SparkSession...\")\n",
    "\n",
    "# create a Spark session that installs Delta Lake onto the Spark Engine remotely\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DeltaLakeIntegration\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-spark_2.12:3.1.0\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "logger.info(\"Setting up Spark configurations...\")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d40d44e1-f9ff-43e1-92b1-b5ec674c8865",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the Doggy Delta Adventure!\n",
      "Initial doggy data successfully added!\n",
      "\n",
      "New dogs are arriving at the park!\n",
      "New doggy friends added to the park!\n",
      "\n",
      "Let's see who's at the park today:\n",
      "+--------+----------+--------+\n",
      "|owner_id|owner_name|dog_name|\n",
      "+--------+----------+--------+\n",
      "|       3|   Charlie|   Bella|\n",
      "|       8|    Hannah|   Daisy|\n",
      "|       7|     Grace|  Cooper|\n",
      "|       1|     Alice|   Buddy|\n",
      "|       5|      Emma|  Bailey|\n",
      "|       6|     Frank|   Rosie|\n",
      "|       4|     David|    Lucy|\n",
      "|       2|       Bob|     Max|\n",
      "+--------+----------+--------+\n",
      "\n",
      "\n",
      "Uh-oh! Trouble's brewing at the park.\n",
      "One mischievous dog has started a commotion!\n",
      "Buddy's owner has been mysteriously changed to 'Mischief'!\n",
      "\n",
      "Time to restore peace and order at the park.\n",
      "Order restored! Buddy is back with his rightful owner.\n",
      "\n",
      "Let's check the park's population after restoring order:\n",
      "+--------+----------+--------+\n",
      "|owner_id|owner_name|dog_name|\n",
      "+--------+----------+--------+\n",
      "|       3|   Charlie|   Bella|\n",
      "|       8|    Hannah|   Daisy|\n",
      "|       7|     Grace|  Cooper|\n",
      "|       1|     Alice|   Buddy|\n",
      "|       5|      Emma|  Bailey|\n",
      "|       6|     Frank|   Rosie|\n",
      "|       4|     David|    Lucy|\n",
      "|       2|       Bob|     Max|\n",
      "+--------+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define delta output path\n",
    "delta_output_path = \"/home/jovyan/data/delta_table_of_dog_owners\"\n",
    "\n",
    "# --- Dogs at the Park Scenario ---\n",
    "\n",
    "print(\"Welcome to the Doggy Delta Adventure!\")\n",
    "\n",
    "# --- Create Test Data and Initial Write to Delta Table ---\n",
    "\n",
    "# It's a bright day at the park. Let's meet our first set of dog owners!\n",
    "owner_data = [Row(owner_id=1, owner_name=\"Alice\", dog_name=\"Buddy\"),\n",
    "              Row(owner_id=2, owner_name=\"Bob\", dog_name=\"Max\"),\n",
    "              Row(owner_id=3, owner_name=\"Charlie\", dog_name=\"Bella\"),\n",
    "              Row(owner_id=4, owner_name=\"David\", dog_name=\"Lucy\"),\n",
    "              Row(owner_id=5, owner_name=\"Emma\", dog_name=\"Bailey\"),\n",
    "              Row(owner_id=6, owner_name=\"Frank\", dog_name=\"Rosie\")]\n",
    "owner_df = spark.createDataFrame(owner_data)\n",
    "\n",
    "# Write initial test data to Delta table\n",
    "owner_df.write.format(\"delta\").mode(\"overwrite\").save(delta_output_path)\n",
    "print(\"Initial doggy data successfully added!\")\n",
    "\n",
    "# --- Additional Dogs Joining the Park ---\n",
    "\n",
    "print(\"\\nNew dogs are arriving at the park!\")\n",
    "new_owner_data = [Row(owner_id=7, owner_name=\"Grace\", dog_name=\"Cooper\"),\n",
    "                  Row(owner_id=8, owner_name=\"Hannah\", dog_name=\"Daisy\")]\n",
    "new_owner_df = spark.createDataFrame(new_owner_data)\n",
    "\n",
    "# Append new test data to the Delta table\n",
    "new_owner_df.write.format(\"delta\").mode(\"append\").save(delta_output_path)\n",
    "print(\"New doggy friends added to the park!\")\n",
    "\n",
    "# --- Let's Take a Look at Our Park's Population ---\n",
    "\n",
    "print(\"\\nLet's see who's at the park today:\")\n",
    "deltaTable = DeltaTable.forPath(spark, delta_output_path)\n",
    "current_df = deltaTable.toDF()\n",
    "current_df.show()\n",
    "\n",
    "# --- Oh No, a Mischievous Dog Causes Trouble! ---\n",
    "\n",
    "print(\"\\nUh-oh! Trouble's brewing at the park.\")\n",
    "print(\"One mischievous dog has started a commotion!\")\n",
    "\n",
    "# Simulate a misbehaving dog by changing its owner\n",
    "deltaTable.update(\"dog_name = 'Buddy'\", {\"owner_name\": \"'Mischief'\"})\n",
    "print(\"Buddy's owner has been mysteriously changed to 'Mischief'!\")\n",
    "current_df.show()\n",
    "\n",
    "# --- Time to Restore Order ---\n",
    "\n",
    "print(\"\\nTime to restore peace and order at the park.\")\n",
    "\n",
    "# Rollback to the previous version to correct the owner of Buddy\n",
    "previous_version = deltaTable.history().select(\"version\").collect()[1][0]\n",
    "deltaTable.restoreToVersion(previous_version)\n",
    "print(\"Order restored! Buddy is back with his rightful owner.\")\n",
    "\n",
    "# --- Let's Check Our Park Population Again ---\n",
    "\n",
    "print(\"\\nLet's check the park's population after restoring order:\")\n",
    "current_df = deltaTable.toDF()\n",
    "current_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789e1eac-b88a-4525-887a-496dabd98504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the SparkSession (you'll have to re-run all the things if you click this)\n",
    "logger.info(\"Stopping SparkSession...\")\n",
    "spark.stop()\n",
    "logger.info(\"SparkSession stopped.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
